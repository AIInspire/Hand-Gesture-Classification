# ğŸ¤– Hand Gesture Classification Using MediaPipe & HaGRID Dataset

This project focuses on classifying 18 different hand gestures using landmark coordinates extracted via **MediaPipe** from the **HaGRID dataset**. A trained machine learning model then classifies the gestures in **real-time using a webcam**.

---

## ğŸ“ Project Structure

```plaintext
Hand-Gesture-Classification/
â”‚
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ haGRID_landmarks.csv          # Preprocessed landmark data from MediaPipe
â”‚
â”œâ”€â”€ best_model.pkl                    # Final trained model
â”œâ”€â”€ label_encoder.pkl                 # Label encoder to map class numbers to labels
â”‚
â”œâ”€â”€ Data_Preparation.ipynb           # Data cleaning, preprocessing, and normalization
â”œâ”€â”€ Training_and_Evaluation.ipynb    # Training and tuning of multiple ML models
â”œâ”€â”€ Implementation_using_Mediapipe.ipynb   # Real-time implementation using MediaPipe & webcam
â”‚
â”œâ”€â”€ README.md                         # Project documentation (youâ€™re here!)

